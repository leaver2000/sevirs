{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 08:52:48.169562: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 08:52:49.582989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import abc\n",
    "from typing import *  # type: ignore\n",
    "\n",
    "from typing_extensions import Self\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    # from keras.api._v2.keras import Model, layers\n",
    "    from keras import Model, layers  # type: ignore\n",
    "else:\n",
    "    from tensorflow.keras import Model, layers  # type: ignore\n",
    "\n",
    "from sevirs._typing import Array, Tensor, Nd, B, C, W, L, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = BATCH, CHANNELS, WIDTH, LENGTH, TIME = (15, 3, 256, 256, 49)\n",
    "a = np.arange(np.prod(SHAPE)).reshape(SHAPE)\n",
    "_T = TypeVar(\"_T\")\n",
    "\n",
    "# def ar() -> Tensor[Nd[B, C, W, L, T], np.int64]:\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOMixin(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def to_disk(self, path: os.PathLike) -> None:\n",
    "        ...\n",
    "\n",
    "    @classmethod\n",
    "    @abc.abstractmethod\n",
    "    def from_disk(cls: Type[Self], path: os.PathLike) -> Self:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndexT = TypeVar(\"IndexT\")\n",
    "ValueT = TypeVar(\"ValueT\")\n",
    "import logging\n",
    "import tqdm\n",
    "import polars as pl\n",
    "from sevirs.utils.sampler import Sampler\n",
    "\n",
    "class DataGenerator(Iterable[ValueT], Generic[IndexT, ValueT]):\n",
    "    def __init__(self, indices: Iterable[IndexT]) -> None:\n",
    "        super().__init__()\n",
    "        self.indices: Final[tuple[IndexT, ...]] = tuple(indices)\n",
    "\n",
    "    # =================================================================================================================\n",
    "    # - abstract methods\n",
    "    @abc.abstractmethod\n",
    "    def get(self, index: IndexT) -> ValueT:\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_metadata(self, img_id: IndexT | None = None) -> pl.DataFrame:\n",
    "        ...\n",
    "\n",
    "    # =================================================================================================================\n",
    "    # - overloads\n",
    "    @overload  # type: ignore[misc]\n",
    "    def select(self, index: IndexT, *, metadata: Literal[False] = False) -> ValueT:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def select(self, index: IndexT, *, metadata: Literal[True] = True) -> tuple[ValueT, pl.DataFrame]:\n",
    "        ...\n",
    "\n",
    "    def select(self, index: IndexT, *, metadata: bool = False) -> ValueT | tuple[ValueT, pl.DataFrame]:\n",
    "        values = self.get(index)\n",
    "        if metadata:\n",
    "            return values, self.get_metadata(index)\n",
    "        return values\n",
    "\n",
    "    @overload  # type: ignore[misc]\n",
    "    def iterate(self, *, metadata: Literal[False] = False) -> Iterable[ValueT]:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def iterate(self, *, metadata: Literal[True] = True) -> Iterable[tuple[ValueT, pl.DataFrame]]:\n",
    "        ...\n",
    "\n",
    "    def iterate(self, *, metadata: bool = False) -> Iterable[ValueT | tuple[ValueT, pl.DataFrame]]:\n",
    "        logging.info(\"ðŸƒ Iterating over Dataset ðŸƒ\")\n",
    "        for index in tqdm.tqdm(self.indices):\n",
    "            yield self.select(index, metadata=metadata)  # type: ignore[call-overload]\n",
    "\n",
    "    # =================================================================================================================\n",
    "    # - dunder methods\n",
    "    def __getitem__(self, idx: int) -> ValueT:\n",
    "        return self.get(self.indices[idx])\n",
    "\n",
    "    def __iter__(self) -> Iterator[ValueT]:\n",
    "        yield from self.iterate(metadata=False)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "class Dataloader(Generic[ValueT]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: DataGenerator[Any, ValueT],\n",
    "        dataset: DataGenerator,\n",
    "        batch_size: int | None = 1,\n",
    "        shuffle: bool | None = None,\n",
    "        sampler: Sampler | Iterable | None = None,\n",
    "        batch_sampler: Sampler[Sequence] | Iterable[Sequence] | None = None,\n",
    "        num_workers: int = 0,\n",
    "        collate_fn: Callable[[list[TensorPair]], Any] | None = None,\n",
    "        pin_memory: bool = False,\n",
    "        drop_last: bool = False,\n",
    "        timeout: float = 0,\n",
    "        worker_init_fn: Callable[[int], None] | None = None,\n",
    "        multiprocessing_context=None,\n",
    "        generator=None,\n",
    "        *,\n",
    "        prefetch_factor: int | None = None,\n",
    "        persistent_workers: bool = False,\n",
    "        pin_memory_device: str = \"\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 08:52:57.742526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:52:57.743747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:52:57.743813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:52:57.745368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:52:57.745481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:52:57.745533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:53:02.320020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:53:02.320192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:53:02.320207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-16 08:53:02.320259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-16 08:53:02.320303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1711 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-08-16 08:53:02.326719: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1156055040 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "class FeatureGenerator(DataGenerator[int, tuple[Tensor, Tensor]]):\n",
    "    def __init__(self, data: Tensor):\n",
    "        super().__init__(range(len(data)))\n",
    "        self.data = data\n",
    "\n",
    "    def get(self, idx) -> tuple[Tensor, Tensor]:\n",
    "        return self.data[idx], self.data[idx + 1]\n",
    "\n",
    "    def get_metadata(self, img_id: int | None = None) -> pl.DataFrame:\n",
    "        ...\n",
    "        # return super().get_metadata(img_id)\n",
    "\n",
    "\n",
    "fg = FeatureGenerator(tf.convert_to_tensor(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class S with abstract methods __getitem__, __len__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mS\u001b[39;00m(Sequence[\u001b[39mint\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m----> 4\u001b[0m S()\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class S with abstract methods __getitem__, __len__"
     ]
    }
   ],
   "source": [
    "class S(Sequence[int]):\n",
    "    ...\n",
    "\n",
    "\n",
    "S()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    \"\"\"\n",
    "    tensorflow generative [autoencoder](https://www.tensorflow.org/tutorials/generative/autoencoder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int, shape: tuple[int, ...]) -> None:\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.shape = shape\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [layers.Dense(tf.math.reduce_prod(shape), activation=\"sigmoid\"), layers.Reshape(shape)]\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        encoded = self.encoder.__call__(x)\n",
    "        decoded = self.decoder.__call__(encoded)\n",
    "        return decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
